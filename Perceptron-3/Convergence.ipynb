{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Perceptron Convergence Theorem\n",
    "\n",
    "We present the convergence theorem in this section\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***If the sets $P$ and $N$ are finite and linearly separable. then the perceptron learning algorithm updates the weight vector, a finit number of times***\n",
    "\n",
    "***Proof:***\n",
    "\n",
    "Before we present the proof, we list the assumption made:\n",
    "\n",
    "1. Let $P^{'}$ be $P \\cup N^{-}$, where $N^{-}$ is the set of negated vectors in $N$\n",
    "\n",
    "    *The only test for $P^{'}$ is that the dot product must be positive*\n",
    "\n",
    "\n",
    "2. Let all the vectors in $P^{'}$ be normalised so that ${\\lvert}\\vec{p_{\\text{i}}}{\\rvert} = 1$\n",
    "\n",
    "\n",
    "3. We assume that the solution weight vector $w^{*}$ exists and that ${\\lvert}\\vec{w^{*}}{\\rvert} = 1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Proof**\n",
    "\n",
    "Assume that after $t + 1$ steps the weight vector is $\\vec{w_{t + 1}}$. This means that at time $t$, some pattern $p_{i} \\ \\epsilon \\ P^{'}$ that was presented at time $t$ was missclassified.\n",
    "\n",
    "$$\n",
    "\\therefore \\vec{w_{t + 1}} = \\vec{w_{t}} + \\vec{p_{i}} \\ (when \\ \\eta = 1) \\ \\text{,where $\\eta$ is the learning rate}\n",
    "$$\n",
    "\n",
    "The cosine of the angle between $\\vec{w_{t+1}}$ and $\\vec{w^{*}}$ is \n",
    "\n",
    "$$\n",
    "\\cos \\rho = \\frac{\\vec{w_{t+1}} * \\vec{w^{*}}}{{\\lvert}\\vec{w_{t+1}}{\\rvert}*{\\lvert}\\vec{w^{*}}{\\rvert}}...(1)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Consider the Numerator\n",
    "\n",
    "$\\vec{w_{t+1}} * \\vec{w^{*}}$\n",
    "\n",
    "$\n",
    " = \\vec{w^{*}} *  (\\vec{w_{t}} + \\vec{p_{i}})\\\\\n",
    " = \\vec{w^{*}} * \\vec{w_{t}} + \\vec{w^{*}} * \\vec{p_{i}}\\\\\n",
    " Let \\ \\delta = \\text{min}\\{\\vec{w^{*}} * \\vec{p_{i}}\\ \\lvert \\ \\forall \\vec{p_{i}} \\ \\epsilon \\ P^{'}\\} \\ and \\ \\delta \\gt 0\\\\\n",
    " \\vec{w_{t+1}} * \\vec{w^{*}} \\ge \\vec{w^{*}} * \\vec{w_{t}} + \\delta \\\\\n",
    " \\ge \\vec{w^{*}} *  (\\vec{w_{t - 1}} + \\vec{p_{j}}) + \\delta \\\\\n",
    " \\ge \\vec{w^{*}} * \\vec{w_{t-1}} + \\vec{w^{*}} * \\vec{p_{j}} + \\delta \\\\\n",
    " \\ge \\vec{w^{*}} * \\vec{w_{t}} + \\delta + \\delta \\\\\n",
    " \\ge \\vec{w^{*}} * \\vec{w_{t}} + 2\\delta \\\\\n",
    " \\therefore By \\ induction \\ \\\\\n",
    " \\vec{w_{t+1}} * \\vec{w^{*}} \\ge \\vec{w^{*}} * \\vec{w_{0}} + (t+1)\\delta ...(2) \\\\ \n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Consider Denominator\n",
    "\n",
    "$\n",
    "{\\lvert}\\vec{w_{t+1}}{\\rvert}*{\\lvert}\\vec{w^{*}}{\\rvert} = {\\lvert}\\vec{w_{t+1}}{\\rvert} \\  (\\because {\\lvert}\\vec{w^{*}}{\\rvert} = 1) \\\\\n",
    "Consider, \\\\ \n",
    "{\\lvert}\\vec{w_{t+1}}{\\rvert}^{2} = (\\vec{w_{t}} + \\vec{p_{i}}) * (\\vec{w_{t}} + \\vec{p_{i}}) \\\\\n",
    "= {\\lvert}\\vec{w_{t}}{\\rvert}^{2} + 2\\vec{w_{t}}\\vec{p_{i}} + {\\lvert}\\vec{p_{i}}{\\rvert}^{2} \\\\\n",
    "= {\\lvert}\\vec{w_{t}}{\\rvert}^{2} + 2\\vec{w_{t}}\\vec{p_{i}} \\ (\\because {\\lvert}\\vec{p_{i}}{\\rvert}^{2} = 1) \\\\\n",
    "$\n",
    "\n",
    "In the above equation, $\\vec{w_{t}}*\\vec{p_{i}}$ is negative. This is because, we did a correction at step $t+1$, which means that the vector $\\vec{w_{t}}$, on dot product with the input pattern must have yielded a negative value.\n",
    "\n",
    "$\\therefore \\vec{w_{t}}\\vec{p_{i}} \\lt 0$ at step $t+1$\n",
    "\n",
    "$\n",
    "\\therefore {\\lvert}\\vec{w_{t+1}}{\\rvert}^{2} \\le {\\lvert}\\vec{w_{t}}{\\rvert}^{2} + 1 \\\\\n",
    "\\le {\\lvert}\\vec{w_{t-1}}{\\rvert}^{2} + 2 \\\\\n",
    "\\therefore By \\ induction \\\\\n",
    "{\\lvert}\\vec{w_{t+1}}{\\rvert}^{2} \\le {\\lvert}\\vec{w_{0}}{\\rvert}^{2} + (t + 1) ...(3)\\\\\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Substitute the eqn (2) and (3) in eqn (1)\n",
    "\n",
    "$\n",
    "1 \\ge \\cos \\rho \\ge \\frac{\\vec{w^{*}} * \\vec{w_{0}} + (t+1)\\delta}{\\sqrt{{\\lvert}\\vec{w_{0}}{\\rvert}^{2} + (t + 1)}} \\\\\n",
    "\\ge \\frac{(t + 1)\\delta}{\\sqrt{(t + 1)}} \\\\\n",
    "\\ge \\delta(\\sqrt{t + 1}) \\\\\n",
    "\\ge \\delta(\\sqrt{t}) \\\\\n",
    "\\therefore 1 \\ge \\delta(\\sqrt{t})\n",
    "\\\\\n",
    "\\\\\n",
    "\\\\\n",
    "t \\le \\frac{1}{\\delta^2}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***The Number of iterations (or update steps) is finite and bounded by $\\frac{1}{\\delta^2}$***\n",
    "\n",
    "Conceptually, $\\delta$ is the distance between the solution vector ($w^*$) and the closest input vector. It gives the width of the feasible region(a hyper cone). Hence, if the width is small we take more steps to reach the solution.\n",
    "\n",
    "The $\\frac{1}{\\delta^2}$ is large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
